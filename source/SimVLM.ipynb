{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a5d9a73f38f24976a1201c2d5051ecac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c73368491ee4881990261ee51eb32fd","IPY_MODEL_ea4631eb8b7e4d33b5684a8231c59f0c","IPY_MODEL_a24875555da24a78925706b4d4ceeb48"],"layout":"IPY_MODEL_dde9bffdc25442f099772f289fce89d0"}},"2c73368491ee4881990261ee51eb32fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad0c348e575f4c1bb9e137f09b1e12cc","placeholder":"​","style":"IPY_MODEL_55fcc0202ca042c6909b85071d4d010f","value":"tokenizer_config.json: 100%"}},"ea4631eb8b7e4d33b5684a8231c59f0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_648b91fbcc754141b638a6a6b5549c08","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9273b3b4e19041649c0d602b5faf4202","value":49}},"a24875555da24a78925706b4d4ceeb48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4671c60a4c5c4db8a6776fafb1d922ab","placeholder":"​","style":"IPY_MODEL_57b3c522fc554264b2de5087ee58858a","value":" 49.0/49.0 [00:00&lt;00:00, 371B/s]"}},"dde9bffdc25442f099772f289fce89d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad0c348e575f4c1bb9e137f09b1e12cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55fcc0202ca042c6909b85071d4d010f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"648b91fbcc754141b638a6a6b5549c08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9273b3b4e19041649c0d602b5faf4202":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4671c60a4c5c4db8a6776fafb1d922ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57b3c522fc554264b2de5087ee58858a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f06d960b15fc49fb84618a4bfa4c97a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d42f9b62c414301b194c9db5ef72ef1","IPY_MODEL_6d25d027a0684c348092fc4f72a10a59","IPY_MODEL_0f7005907c7140be8caf2c3454a13fb0"],"layout":"IPY_MODEL_4a045e987dad4b70b90f4f14e90809cb"}},"8d42f9b62c414301b194c9db5ef72ef1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ed381e621174b48a776b3464fd20c52","placeholder":"​","style":"IPY_MODEL_5c85006e411e4c07b70bb4eb24767969","value":"config.json: 100%"}},"6d25d027a0684c348092fc4f72a10a59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_222bb16dd3214a45a9cd65ec37d144b2","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea29f4c2476a40b0a6e343716f5c3771","value":570}},"0f7005907c7140be8caf2c3454a13fb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe61a9ec64e94c61bb9d714046e5cb57","placeholder":"​","style":"IPY_MODEL_3fe8754e6fad4aabaa57dc72aaee9c60","value":" 570/570 [00:00&lt;00:00, 9.98kB/s]"}},"4a045e987dad4b70b90f4f14e90809cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed381e621174b48a776b3464fd20c52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c85006e411e4c07b70bb4eb24767969":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"222bb16dd3214a45a9cd65ec37d144b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea29f4c2476a40b0a6e343716f5c3771":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe61a9ec64e94c61bb9d714046e5cb57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fe8754e6fad4aabaa57dc72aaee9c60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba81ba172ae24162a24afddbb64cf24c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1aa8a7313f8844de8113f2b98a183966","IPY_MODEL_1c23b7adc9d742f49a8a787913c626f4","IPY_MODEL_9c0af3dcab4b44acb7fec7b001cb7c41"],"layout":"IPY_MODEL_56c20b67daca491a916191e669e04252"}},"1aa8a7313f8844de8113f2b98a183966":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c87eedf6e19a4a4c901bec09e23e0fd3","placeholder":"​","style":"IPY_MODEL_e47216a3bf624e55ab4729c512053116","value":"vocab.txt: 100%"}},"1c23b7adc9d742f49a8a787913c626f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aa54310e0074e1ba669014e6d3b0724","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fabe119daac0457d9a6eee8e4ab6f6c9","value":213450}},"9c0af3dcab4b44acb7fec7b001cb7c41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ec60c761dc641c78e6005c664d36898","placeholder":"​","style":"IPY_MODEL_6062ea3e814c4f60aa8d88c8b77b33a2","value":" 213k/213k [00:00&lt;00:00, 1.28MB/s]"}},"56c20b67daca491a916191e669e04252":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c87eedf6e19a4a4c901bec09e23e0fd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e47216a3bf624e55ab4729c512053116":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5aa54310e0074e1ba669014e6d3b0724":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fabe119daac0457d9a6eee8e4ab6f6c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ec60c761dc641c78e6005c664d36898":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6062ea3e814c4f60aa8d88c8b77b33a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9e99175150d47d5a5f8bd5d8bc5d711":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01f10b94373447c985fa27ede1679d9a","IPY_MODEL_3228bbcb103a437b81a626de13d1ad89","IPY_MODEL_fbb17df2894941eb9aed07120d798e2f"],"layout":"IPY_MODEL_b7644c86d2924521bc267231363a6df6"}},"01f10b94373447c985fa27ede1679d9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_582dcefa75014cf189be1b03f9948519","placeholder":"​","style":"IPY_MODEL_be9c3b6eee1146a48ca6cb8b87b8ee3e","value":"tokenizer.json: 100%"}},"3228bbcb103a437b81a626de13d1ad89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87be73fac6e147af8c4b79b9b956c9fe","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5669f2afd4b64dc49e439cf5a3ff2de3","value":435797}},"fbb17df2894941eb9aed07120d798e2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d536e37a5564f1f8808da1d9409cb73","placeholder":"​","style":"IPY_MODEL_4e5d31c2c48649818006bf79e65a7004","value":" 436k/436k [00:00&lt;00:00, 3.20MB/s]"}},"b7644c86d2924521bc267231363a6df6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"582dcefa75014cf189be1b03f9948519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be9c3b6eee1146a48ca6cb8b87b8ee3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87be73fac6e147af8c4b79b9b956c9fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5669f2afd4b64dc49e439cf5a3ff2de3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d536e37a5564f1f8808da1d9409cb73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5d31c2c48649818006bf79e65a7004":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"hwVUSrwwfD-r","executionInfo":{"status":"ok","timestamp":1712107930677,"user_tz":-420,"elapsed":24164,"user":{"displayName":"Hoàng Minh Lưu","userId":"07301216350884297682"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18\n","\n","class SimVLM(nn.Module):\n","    def __init__(self, batch_size, num_patch, num_embedding, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n","        super().__init__()\n","\n","        # モデルのパラメータを初期化\n","        self.batch_size = batch_size\n","        self.seq_length = seq_length\n","        self.hidden_size = hidden_size\n","        self.num_layer = num_layer\n","        self.num_heads = num_heads\n","        self.ffn_hidden_size = ffn_hidden_size\n","        self.num_patch = num_patch\n","\n","        # エンコーダーおよびデコーダーの埋め込み層を定義\n","        self.encoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n","        self.decoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n","\n","        # 位置エンベディングの初期化\n","        self._setupPositionalEmbedding(num_patch, seq_length, hidden_size)\n","\n","        # ResNetモデルの初期化\n","        self.resnet = ResNet(num_patch, hidden_size)\n","\n","        # エンコーダーとデコーダーの初期化\n","        self.encoder = Encoder(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n","        self.decoder = Decoder(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n","\n","    def _setupPositionalEmbedding(self, num_patch, seq_length, hidden_size):\n","        # 位置エンベディングを初期化するメソッド\n","        image_positional_embedding_module = nn.Embedding(num_patch, hidden_size)\n","        encoder_positional_embedding_module = nn.Embedding(seq_length[0], hidden_size)\n","        decoder_positional_embedding_module = nn.Embedding(seq_length[1], hidden_size)\n","\n","        image_position_ids = torch.tensor(list(range(num_patch))).expand(self.batch_size, -1)\n","        encoder_position_ids = torch.tensor(list(range(seq_length[0]))).expand(self.batch_size, -1)\n","        decoder_position_ids = torch.tensor(list(range(seq_length[1]))).expand(self.batch_size, -1)\n","\n","        # 画像、エンコーダー、デコーダーそれぞれの位置エンベディングを初期化\n","        self.image_positional_embs = image_positional_embedding_module(image_position_ids)\n","        self.encoder_positional_embs = encoder_positional_embedding_module(encoder_position_ids)\n","        self.decoder_positional_embs = decoder_positional_embedding_module(decoder_position_ids)\n","\n","    def forward(self, images, encoder_input_ids, decoder_input_ids):\n","        # モデルのフォワードパスを定義\n","\n","        # 画像からエンコーダーへの処理\n","        encoder_image_output = self.resnet(images) + self.image_positional_embs\n","\n","        # エンコーダーのトークン埋め込みと位置エンベディングの結合\n","        encoder_tokens_embedding = self.encoder_embedding(encoder_input_ids) + self.encoder_positional_embs\n","        encoder_concat_tokens = torch.cat([encoder_image_output, encoder_tokens_embedding], dim=1)\n","\n","        # エンコーダーの処理\n","        encoder_output = self.encoder(encoder_concat_tokens)\n","\n","        # デコーダーのトークン埋め込みの取得\n","        decoder_embedding = self.decoder_embedding(decoder_input_ids)\n","\n","        # デコーダーの処理\n","        decoder_output = self.decoder(decoder_embedding, encoder_output)\n","\n","        return decoder_output\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, num_patch, output_size):\n","        super().__init__()\n","        self.num_patch = num_patch\n","        self._initModel(output_size)\n","\n","    def _initModel(self, output_size):\n","        # 事前学習済みのResNet-18モデルをロード\n","        model = resnet18(pretrained=True)\n","        in_features = 1\n","\n","        # 入力チャネル数を調整\n","        conv1_in_channels = self.num_patch\n","        conv1_out_channels = model.conv1.out_channels\n","        model.conv1 = nn.Conv2d(conv1_in_channels, conv1_out_channels, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","\n","        # 逆畳み込み層の入力チャネル数を調整\n","        deconv_in_channels = model.layer3[1].conv2.out_channels\n","        deconv = nn.Conv2d(deconv_in_channels, self.num_patch, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","\n","        # 線形層を設定\n","        self.fc = nn.Linear(in_features, output_size)\n","\n","        # モデルの構築\n","        my_model = nn.Sequential(model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3, deconv, model.avgpool)\n","        self.model = my_model\n","\n","    def patchImages(self, images):\n","        # 画像をトークンに変換する関数\n","        batch_size, num_channel, width, height = images.shape\n","        patch_size = int(width / ((self.num_patch / num_channel) ** 0.5))\n","        patch_window = torch.ones((patch_size, patch_size), dtype=torch.long)\n","\n","        # パッチウィンドウを画像のチャネルに拡張\n","        patch_window = patch_window.unsqueeze(0).expand(num_channel, patch_size, patch_size) \\\n","            .unsqueeze(0).expand(batch_size, num_channel, patch_size, patch_size)\n","\n","        token_list = []\n","\n","        # 画像をパッチに分割\n","        for row_idx in range(0, width, patch_size):\n","            for col_idx in range(0, height, patch_size):\n","                patch = images[:, :, row_idx: row_idx + patch_size, col_idx: col_idx + patch_size]\n","                token_list.append(patch)\n","\n","        # パッチをスタックし、形状を整える\n","        patched_images = torch.stack(token_list, dim=0).transpose(0, 1).reshape(batch_size, self.num_patch, patch_size, patch_size)\n","\n","        return patched_images\n","\n","    def forward(self, images):\n","        patched_images = self.patchImages(images)\n","        image_tokens = self.model(patched_images)\n","        image_tokens = image_tokens.reshape(-1, self.num_patch, 1)\n","        image_tokens = self.fc(image_tokens)\n","\n","        return image_tokens\n","\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size):\n","        super().__init__()\n","\n","        # Multi-Head Attention レイヤー\n","        self.multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=True, check_mask=False)\n","\n","        # Add & Norm レイヤー1\n","        self.add_norm1 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=True)\n","\n","        # FeedForward レイヤー\n","        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n","\n","        # Add & Norm レイヤー2\n","        self.add_norm2 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=True)\n","\n","    def forward(self, tokens):\n","        # 入力トークンを保持しておく（Skip Connection用）\n","        skip1 = tokens\n","\n","        # Multi-Head Attention レイヤーの処理\n","        multi_head_attention = self.multi_head_attention(tokens, tokens, tokens)\n","\n","        # Add & Norm レイヤー1の処理\n","        add_norm1 = self.add_norm1(multi_head_attention, skip1)\n","\n","        # Skip Connectionを保持しておく\n","        skip2 = add_norm1\n","\n","        # FeedForward レイヤーの処理\n","        feed_forward = self.feed_forward(add_norm1)\n","\n","        # Add & Norm レイヤー2の処理\n","        add_norm2 = self.add_norm2(feed_forward, skip2)\n","\n","        # 処理結果を返す\n","        tokens = add_norm2\n","\n","        return tokens\n","\n","class Encoder(nn.Module):\n","    def __init__(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n","        super().__init__()\n","\n","        # 複数のエンコーダーレイヤーを構築\n","        self._setupEncoderLayer(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n","\n","    def _setupEncoderLayer(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n","        # エンコーダーレイヤーをリストとして保持するためのモジュール\n","        encoder_layer_list = []\n","\n","        # 指定された数だけエンコーダーレイヤーを構築\n","        for _ in range(num_layer):\n","            encoder_layer = EncoderLayer(batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size)\n","            encoder_layer_list.append(encoder_layer)\n","\n","        # モジュールリストとしてエンコーダーレイヤーを保持\n","        self.encoder_module = nn.ModuleList(encoder_layer_list)\n","\n","    def forward(self, encoder_embedding):\n","        # 入力トークンを保持しておく\n","        tokens = encoder_embedding\n","\n","        # 各エンコーダーレイヤーを順次適用\n","        for encoder_layer in self.encoder_module:\n","            tokens = encoder_layer(tokens)\n","\n","        # エンコーダーの出力を返す\n","        encoder_output = tokens\n","\n","        return encoder_output\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size):\n","        super().__init__()\n","\n","        # マスク付きのMulti-Head Attention レイヤー\n","        self.masked_multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=False, check_mask=True)\n","\n","        # Add & Norm レイヤー1\n","        self.add_norm1 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n","\n","        # Cross-Attention レイヤー\n","        self.cross_multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=False, check_mask=False)\n","\n","        # Add & Norm レイヤー2\n","        self.add_norm2 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n","\n","        # FeedForward レイヤー\n","        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n","\n","        # Add & Norm レイヤー3\n","        self.add_norm3 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n","\n","    def forward(self, tokens, output_encoder):\n","        # 入力トークンを保持しておく\n","        skip1 = tokens\n","\n","        # マスク付きのMulti-Head Attention レイヤーの処理\n","        masked_multi_head_attention = self.masked_multi_head_attention(tokens, tokens, tokens)\n","\n","        # Add & Norm レイヤー1の処理\n","        add_norm1 = self.add_norm1(masked_multi_head_attention, skip1)\n","\n","        # Skip Connectionを保持しておく\n","        skip2 = add_norm1\n","\n","        # Cross-Attention レイヤーの処理\n","        cross_multi_head_attention = self.cross_multi_head_attention(tokens, output_encoder, output_encoder)\n","\n","        # Add & Norm レイヤー2の処理\n","        add_norm2 = self.add_norm2(cross_multi_head_attention, skip2)\n","\n","        # Skip Connectionを保持しておく\n","        skip3 = add_norm2\n","\n","        # FeedForward レイヤーの処理\n","        feed_forward = self.feed_forward(tokens)\n","\n","        # Add & Norm レイヤー3の処理\n","        add_norm3 = self.add_norm3(feed_forward, skip3)\n","\n","        # 処理結果を返す\n","        tokens = add_norm3\n","\n","        return tokens\n","\n","class Decoder(nn.Module):\n","    def __init__(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n","        super().__init__()\n","\n","        # 複数のデコーダーレイヤーを構築\n","        self._setupDecoderLayer(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n","\n","    def _setupDecoderLayer(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n","        # デコーダーレイヤーをリストとして保持するためのモジュール\n","        decoder_layer_list = []\n","\n","        # 指定された数だけデコーダーレイヤーを構築\n","        for _ in range(num_layer):\n","            decoder_layer = DecoderLayer(batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size)\n","            decoder_layer_list.append(decoder_layer)\n","\n","        # モジュールリストとしてデコーダーレイヤーを保持\n","        self.decoder_module = nn.ModuleList(decoder_layer_list)\n","\n","    def forward(self, decoder_embedding, encoder_output):\n","        # 入力トークンを保持しておく\n","        tokens = decoder_embedding\n","\n","        # 各デコーダーレイヤーを順次適用\n","        for decoder_layer in self.decoder_module:\n","            tokens = decoder_layer(tokens, encoder_output)\n","\n","        # デコーダーの出力を返す\n","        decoder_output = tokens\n","\n","        return decoder_output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding, check_mask):\n","        super().__init__()\n","\n","        # Query、Key、Value用の線形変換モジュールを構築\n","        self._setupHeadQKV(num_heads, hidden_size)\n","\n","        # モジュールのパラメータや設定を保持\n","        self.batch_size = batch_size\n","        self.num_patch = num_patch\n","        self.num_heads = num_heads\n","        self.seq_length = seq_length\n","        self.hidden_size = hidden_size\n","        self.check_positional_embedding = check_positional_embedding\n","        self.check_mask = check_mask\n","\n","        # Softmax関数（Attentionの重み計算用）\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def _setupHeadQKV(self, num_heads, hidden_size):\n","        # Query、Key、Value用のモジュールをリストとして保持するためのモジュール\n","        query_module = []\n","        key_module = []\n","        value_module = []\n","\n","        # ヘッドごとの隠れ層サイズ\n","        head_hidden_size = int(hidden_size / num_heads)\n","\n","        # 指定されたヘッド数だけQuery、Key、Value用のモジュールを構築\n","        for _ in range(num_heads):\n","            query_module.append(nn.Linear(hidden_size, head_hidden_size))\n","            key_module.append(nn.Linear(hidden_size, head_hidden_size))\n","            value_module.append(nn.Linear(hidden_size, head_hidden_size))\n","\n","        # モジュールリストとして保持\n","        self.query_module = nn.ModuleList(query_module)\n","        self.key_module = nn.ModuleList(key_module)\n","        self.value_module = nn.ModuleList(value_module)\n","\n","    def _outputRelativePositionalEmbeddingScalar(self, query, batch_size, num_patch, seq_length, hidden_size, num_heads):\n","        # 相対位置エンベディングを計算する関数\n","\n","        # 入力のシーケンス長を取得\n","        seq_length = seq_length[0]\n","\n","        # Embeddingモジュールを保持するためのリスト\n","        embed_Module = []\n","\n","        # ヘッドごとの隠れ層サイズ\n","        head_hidden_size = int(hidden_size / num_heads)\n","\n","        # 位置情報のIDを作成\n","        position_ids = torch.tensor(list(range(num_patch + seq_length)), dtype=torch.long).reshape(1, num_patch + seq_length).expand(batch_size, num_patch + seq_length)\n","\n","        # 各ヘッドごとにEmbeddingモジュールを構築\n","        for id in range(num_heads):\n","            embed_Module.append(nn.Embedding(num_patch + seq_length, head_hidden_size))\n","\n","        # モジュールリストとしてEmbeddingモジュールを保持\n","        self.embed_module = nn.ModuleList(embed_Module)\n","\n","        # 各ヘッドごとに相対位置エンベディングを計算\n","        for id in range(num_heads):\n","            head_query = self.query_module[id](query)\n","            tmp_relative_position_embedding_scalar = (head_query@(self.embed_module[id](position_ids).transpose(1, 2)))\\\n","                .reshape(1, batch_size, num_patch + seq_length, num_patch + seq_length)[:, :, :num_patch, :num_patch]\n","\n","            # 列のパディングを追加\n","            col_pad = torch.zeros((1, batch_size, num_patch, seq_length), dtype=torch.float)\n","            tmp_relative_position_embedding_scalar = torch.cat([tmp_relative_position_embedding_scalar, col_pad], dim=3)\n","\n","            # 行のパディングを追加\n","            row_pad = torch.zeros((1, batch_size, seq_length, num_patch + seq_length))\n","            tmp_relative_position_embedding_scalar = torch.cat([tmp_relative_position_embedding_scalar, row_pad], dim=2)\n","\n","            # 初めて計算するヘッドの場合は、相対位置エンベディングをそのまま保持\n","            if id == 0:\n","                relative_position_embedding_scalar = tmp_relative_position_embedding_scalar\n","            else:\n","                # すでに計算済みのヘッドがある場合は、テンソルを連結\n","                relative_position_embedding_scalar = torch.cat([relative_position_embedding_scalar, tmp_relative_position_embedding_scalar], dim=0)\n","\n","        return relative_position_embedding_scalar\n","\n","    def _outputAttention(self, query, key, value, batch_size, num_patch, seq_length, hidden_size, num_heads, check_positional_embedding, check_mask):\n","        # Attentionスコアを計算する関数\n","\n","        # マスクや位置エンベディングの有無によって、処理を分岐\n","        if check_positional_embedding:\n","            seq_length1 = seq_length2 = num_patch + self.seq_length[0]\n","        else:\n","            if check_mask:\n","                seq_length1 = seq_length2 = self.seq_length[1]\n","            else:\n","                seq_length1 = self.seq_length[1]\n","                seq_length2 = num_patch + self.seq_length[0]\n","\n","        # ヘッドごとの隠れ層サイズ\n","        head_hidden_size = int(hidden_size / num_heads)\n","\n","        # マスクマップを作成\n","        mask_map = torch.tensor(np.tril(np.ones((seq_length1, seq_length2))), dtype=torch.long)\n","\n","        # 位置エンベディングが指定されている場合、相対位置エンベディングを計算\n","        if check_positional_embedding:\n","            relative_position_embedding_scalar = self._outputRelativePositionalEmbeddingScalar(query, batch_size, num_patch, seq_length, hidden_size, num_heads)\n","        else:\n","            relative_position_embedding_scalar = None\n","\n","        # 各ヘッドごとにAttentionスコアを計算\n","        for id in range(num_heads):\n","            head_query = self.query_module[id](query)\n","            head_key = self.key_module[id](key)\n","            head_value = self.value_module[id](value)\n","\n","            if check_positional_embedding:\n","                # 位置エンベディングが指定されている場合、Attentionスコアに相対位置エンベディングを加算\n","                tmp_head_attention = self.softmax(((head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id]))@head_value\n","            else:\n","                # 位置エンベディングが指定されていない場合\n","                if check_mask:\n","                    # マスクが指定されている場合、Attentionスコアにマスクを適用\n","                    tmp_head_attention = self.softmax((mask_map * (head_query@head_key.transpose(1, 2)) / (head_hidden_size)))@head_value\n","                else:\n","                    # マスクが指定されておらず、位置エンベディングもない場合、通常のAttentionスコア計算\n","                    tmp_head_attention = self.softmax((head_query@head_key.transpose(1, 2)) / (head_hidden_size))@head_value\n","\n","            # はじめて計算するヘッドの場合は、Attentionスコアをそのまま保持\n","            if id == 0:\n","                head_attention = tmp_head_attention\n","            else:\n","                # すでに計算済みのヘッドがある場合は、テンソルを連結\n","                head_attention = torch.cat([head_attention, tmp_head_attention], dim=-1)\n","\n","        # 出力のAttentionスコアを返す\n","        output_attention = head_attention\n","\n","        return output_attention\n","\n","    def forward(self, query, key, value):\n","        # フォワード関数\n","\n","        # Attentionスコアを計算\n","        output_attention = self._outputAttention(query, key, value, self.batch_size, self.num_patch, self.seq_length, self.hidden_size, self.num_heads,\n","                                                  self.check_positional_embedding, self.check_mask)\n","\n","        return output_attention\n","\n","class AddNorm(nn.Module):\n","    def __init__(self, batch_size, num_patch, seq_length, hidden_size, check_encoder):\n","        super().__init__()\n","\n","        # AddNormモジュールを構築\n","        self._setupAddNormModule(batch_size, num_patch, seq_length, hidden_size, check_encoder)\n","\n","    def _setupAddNormModule(self, batch_size, num_patch, seq_length, hidden_size, check_encoder):\n","        # エンコーダーの場合、シーケンス長はエンコーダーのシーケンス長とパッチ数の合計\n","        if check_encoder:\n","            seq_length = seq_length[0] + num_patch\n","        else:\n","            # デコーダーの場合、シーケンス長はデコーダーのシーケンス長\n","            seq_length = seq_length[1]\n","\n","        # LayerNormモジュールを構築\n","        self.layer_norm = nn.LayerNorm((batch_size, seq_length, hidden_size))\n","\n","    def forward(self, tokens, skipped_tokens):\n","        # 入力トークンにスキップしたトークンを加算\n","        tokens += skipped_tokens\n","\n","        # LayerNormを適用して出力トークンを生成\n","        tokens = self.layer_norm(tokens)\n","\n","        return tokens\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, hidden_size, ffn_hidden_size):\n","        super().__init__()\n","\n","        # FeedForwardモジュールを構築\n","        self._setupFeedForwardModule(hidden_size, ffn_hidden_size)\n","\n","    def _setupFeedForwardModule(self, hidden_size, ffn_hidden_size):\n","        # 1つ目の全結合層とReLU活性化関数\n","        dense1 = nn.Linear(hidden_size, ffn_hidden_size)\n","        relu1 = nn.ReLU()\n","\n","        # 2つ目の全結合層とReLU活性化関数\n","        dense2 = nn.Linear(ffn_hidden_size, hidden_size)\n","        relu2 = nn.ReLU()\n","\n","        # モジュールリストとして保持\n","        self.feed_foward_module = nn.ModuleList([dense1, relu1, dense2, relu2])\n","\n","    def forward(self, tokens):\n","        # フォワード関数\n","\n","        # モジュールリスト内の各モジュールを順に適用\n","        for module in self.feed_foward_module:\n","            tokens = module(tokens)\n","\n","        # 出力トークンを返す\n","        return tokens"]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import numpy as np\n","\n","# BERTトークナイザーの読み込み\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") # T5Tokenizerでは文頭トークン<s>が表示されなかったため、bertで代用\n","\n","# 入力となる英語のトークン化\n","enocoder_input_tokens = [\"Translate English to German : Two brown and white dogs\",\n","                         \"Translate English to German : The man plaing soccoer\",\n","                         \"Translate English to German : The birds is flying\"]\n","encoder_input_tokenize = tokenizer(enocoder_input_tokens, padding=True, return_tensors=\"pt\", return_length=True)\n","encoder_input_ids = encoder_input_tokenize[\"input_ids\"][:, 1:]  # 先頭のトークン<s>を除外\n","encoder_attention_mask = encoder_input_tokenize[\"attention_mask\"][:, 1:]\n","encoder_attention_mask = torch.where(encoder_input_ids == 102, 0, encoder_attention_mask)  # 文末トークンを除外\n","encoder_input_ids = torch.where(encoder_input_ids == 102, 0, encoder_input_ids)  # 文末トークンを除外\n","\n","# 出力となるドイツ語のトークン化\n","decoder_input_tokens = [\"Zwei braune und weiße Hunde\",\n","                       \"Der Mann spielt Fußball\",\n","                       \"Die Vögel fliegen\"]\n","decoder_input_tokenize = tokenizer(decoder_input_tokens, padding=True, return_tensors=\"pt\", return_length=True)\n","decoder_input_ids = decoder_input_tokenize[\"input_ids\"]\n","decoder_attention_mask = decoder_input_tokenize[\"attention_mask\"]\n","\n","encoder_max_length = encoder_input_tokenize[\"length\"][0].item() - 1\n","decoder_max_length = decoder_input_tokenize[\"length\"][0].item()\n","\n","# 画像の生成と前処理\n","images = torch.randn(3, 3, 256, 256, dtype=torch.float)\n","patch_size = 16\n","num_patch = int((images.shape[2] / patch_size) ** 2) * images.shape[1]\n","num_embedding = torch.max(torch.concat([encoder_input_ids, decoder_input_ids], dim=-1)) + 1\n","\n","# SimVLMモデルの初期化と実行\n","kwargs = {\n","    \"batch_size\": 3,\n","    \"num_patch\": num_patch,\n","    \"num_embedding\": num_embedding,\n","    \"seq_length\": (encoder_max_length, decoder_max_length),\n","    \"hidden_size\": 512,\n","    \"num_layer\": 12,\n","    \"num_heads\": 8,\n","    \"ffn_hidden_size\": 3072\n","}\n","simvlm = SimVLM(**kwargs)\n","\n","# モデルの出力の形状を表示\n"],"metadata":{"id":"Y-0uR5OffIMg","colab":{"base_uri":"https://localhost:8080/","height":373,"referenced_widgets":["a5d9a73f38f24976a1201c2d5051ecac","2c73368491ee4881990261ee51eb32fd","ea4631eb8b7e4d33b5684a8231c59f0c","a24875555da24a78925706b4d4ceeb48","dde9bffdc25442f099772f289fce89d0","ad0c348e575f4c1bb9e137f09b1e12cc","55fcc0202ca042c6909b85071d4d010f","648b91fbcc754141b638a6a6b5549c08","9273b3b4e19041649c0d602b5faf4202","4671c60a4c5c4db8a6776fafb1d922ab","57b3c522fc554264b2de5087ee58858a","f06d960b15fc49fb84618a4bfa4c97a2","8d42f9b62c414301b194c9db5ef72ef1","6d25d027a0684c348092fc4f72a10a59","0f7005907c7140be8caf2c3454a13fb0","4a045e987dad4b70b90f4f14e90809cb","2ed381e621174b48a776b3464fd20c52","5c85006e411e4c07b70bb4eb24767969","222bb16dd3214a45a9cd65ec37d144b2","ea29f4c2476a40b0a6e343716f5c3771","fe61a9ec64e94c61bb9d714046e5cb57","3fe8754e6fad4aabaa57dc72aaee9c60","ba81ba172ae24162a24afddbb64cf24c","1aa8a7313f8844de8113f2b98a183966","1c23b7adc9d742f49a8a787913c626f4","9c0af3dcab4b44acb7fec7b001cb7c41","56c20b67daca491a916191e669e04252","c87eedf6e19a4a4c901bec09e23e0fd3","e47216a3bf624e55ab4729c512053116","5aa54310e0074e1ba669014e6d3b0724","fabe119daac0457d9a6eee8e4ab6f6c9","6ec60c761dc641c78e6005c664d36898","6062ea3e814c4f60aa8d88c8b77b33a2","c9e99175150d47d5a5f8bd5d8bc5d711","01f10b94373447c985fa27ede1679d9a","3228bbcb103a437b81a626de13d1ad89","fbb17df2894941eb9aed07120d798e2f","b7644c86d2924521bc267231363a6df6","582dcefa75014cf189be1b03f9948519","be9c3b6eee1146a48ca6cb8b87b8ee3e","87be73fac6e147af8c4b79b9b956c9fe","5669f2afd4b64dc49e439cf5a3ff2de3","5d536e37a5564f1f8808da1d9409cb73","4e5d31c2c48649818006bf79e65a7004"]},"executionInfo":{"status":"ok","timestamp":1712107944139,"user_tz":-420,"elapsed":13464,"user":{"displayName":"Hoàng Minh Lưu","userId":"07301216350884297682"}},"outputId":"e9c15817-a87a-4d2a-e0dd-a6f72e0bb485"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d9a73f38f24976a1201c2d5051ecac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06d960b15fc49fb84618a4bfa4c97a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba81ba172ae24162a24afddbb64cf24c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e99175150d47d5a5f8bd5d8bc5d711"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:01<00:00, 29.2MB/s]\n"]}]},{"cell_type":"code","source":["print(encoder_max_length, decoder_max_length)\n","print(simvlm(images, encoder_input_ids, decoder_input_ids).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_iM6QJqf-Kh","executionInfo":{"status":"ok","timestamp":1712107961706,"user_tz":-420,"elapsed":17581,"user":{"displayName":"Hoàng Minh Lưu","userId":"07301216350884297682"}},"outputId":"d001827b-bcbd-4f6d-e651-313bd6553886"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["14 12\n","torch.Size([3, 12, 512])\n"]}]}]}